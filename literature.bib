@book{1,
  title={Multimediale und telemediale Lernumgebungen: Konzeption und Entwicklung},
  author={Kerres, Michael},
  year={2001},
  publisher={Walter de Gruyter}
}

@article{2,
 author = {Downes, Stephen},
 title = {E-learning 2.0},
 journal = {eLearn},
 issue_date = {October 2005},
 volume = {2005},
 number = {10},
 month = oct,
 year = {2005},
 issn = {1535-394X},
 pages = {1--},
 url = {http://doi.acm.org/10.1145/1104966.1104968},
 doi = {10.1145/1104966.1104968},
 acmid = {1104968},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@misc{3,
  author       = {O'Reilly, Tim},
  URL          = {https://www.oreilly.com/pub/a/web2/archive/what-is-web-20.html},
  title        = {What Is Web 2.0},
  year         = {2005},
}

@book{4,
  title={We the media: Grassroots journalism by the people, for the people},
  author={Gillmor, Dan},
  year={2006},
  publisher={ O'Reilly Media, Inc.}
}

@misc{5,
  author       = {Nolan Richard},
  title        = {Advantages and Disadvantages of E-Learning Technologies for Students},
  URL          = {https://www.joomlalms.com/blog/guest-posts/elearning-advantages-disadvantages.html},
  year         = {2017},
}

@misc{6,
  author       = {Ferriman, Justin},
  publisher    = {LearnDash},
  title        = {7 Awesome Advantages of ELearning},
  URL          = {https://www.learndash.com/7-awesome-advantages-of-elearning/},
  year         = {2013},
}

@misc{7,
  author       = {Angelika, Mair and Janine, Butzerin},
  title        = {E-Learning},
  location     = {Innsbruck},
  URL          = {https://www.uibk.ac.at/psychologie/mitarbeiter/leidlmair/seminararbeit_e-learning.pdf},
  year         = {2011},
}

@misc{8,
  author       = {Thalheimer, Will},
  title        = {Does eLearning Work? What the Scientific Research Says!},
  URL          = {https://www.worklearning.com/wp-content/uploads/2017/10/Does-eLearning-Work-Full-Research-Report-FINAL2.pdf},
  year         = {2017},
}

@misc{9,
  author       = {Pfeifer, Thies},
  title        = {Virtual Reality Technology},
  year         = {2017},
}

@article{10,
  title={Dual coding theory and education},
  author={Clark, James M and Paivio, Allan},
  journal={Educational psychology review},
  volume={3},
  number={3},
  pages={149--210},
  year={1991},
  publisher={Springer}
}

@article{11,
  title={Emotional stress and eyewitness memory: a critical review.},
  author={Christianson, Sven-{\AA}ke},
  journal={Psychological bulletin},
  volume={112},
  number={2},
  pages={284},
  year={1992},
  publisher={American Psychological Association}
}

@article{12,
  title={Analysis of assets for virtual reality applications in neuropsychology},
  author={Rizzo, Albert A and Schultheis, Maria and Kerns, Kimberly A and Mateer, Catherine},
  journal={Neuropsychological rehabilitation},
  volume={14},
  number={1-2},
  pages={207--239},
  year={2004},
  publisher={Taylor \& Francis}
}

@article{13,
  title={Virtual reality in neuroscience research and therapy},
  author={Bohil, Corey J and Alicea, Bradly and Biocca, Frank A},
  journal={Nature reviews neuroscience},
  volume={12},
  number={12},
  pages={752},
  year={2011},
  publisher={Nature Publishing Group}
}

@misc{14,
  author       = {Rouse Margaret},
  title        = {virtual reality-based training (VRBT)},
  URL          = {https://whatis.techtarget.com/definition/virtual-reality-based-training-VRBT},
  year         = {2017},
}

@misc{15,
  author       = {Gr. Baptiste},
  title        = {WHAT IS VIRTUAL REALITY TRAINING AND WHAT ARE ITS ADVANTAGES?},
  URL          = {https://provr.io/blog/vr-training/},
  year         = {2018},
}

@misc{16,
  author       = {Simblog},
  title        = {8 Advantages Of Virtual Reality Trainings},
  URL          = {http://www.simlabit.com/blog/virtual-reality/vr-training/advantages-of-virtual-reality-trainings/},
  year         = {2017},
}

@misc{17,
  author       = {Hicks, Paula},
  title        = {The Pros And Cons Of Using Virtual Reality In The Classroom},
  URL          = {https://elearningindustry.com/pros-cons-using-virtual-reality-in-the-classroom},
  year         = {2016},
}

@misc{18,
  author       = {Casale, Michael and Willage, Joe},
  title        = {Attention Analytics \& VR Training},
  URL          = {https://www.strivr.com/wp-content/uploads/2018/10/STRIVR_Attention-Analytics.pdf},
  year         = {2018},
}

@inproceedings{19,
  title={Virtual reality overview},
  author={Cruz-Neira, Carolina},
  booktitle={SIGGRAPH},
  volume={93},
  pages={1--1},
  year={1993}
}

@misc{20,
  author       = {NovaVision},
  title        = {Wie funktioniert das räumliche Sehen?},
  URL          = {https://novavision.de/raeumlich-sehen/},
  year         = {2016},
}

@misc{21,
  author       = {webvr.info},
  URL          = {https://webvr.info/},
}

@misc{22,
  author       = {APP-ETWICKLER-VERZEICHNIS},
  title        = {Native Apps vs. Web Apps - Unterschiede und Vorteile},
  URL          = {https://app-entwickler-verzeichnis.de/faq-app-entwicklung/11-definitionen/586-unterschiede-und-vergleich-native-apps-vs-web-apps-2},
  year         = {2017},
}

@misc{23,
  author       = {Khronos},
  title        = {WebGL Overview},
  URL          = {https://www.khronos.org/webgl/},
}

@misc{24,
  author       = {Incao, Jane},
  title        = {How VR is Transforming the Way We Train Associates},
  URL          = {https://blog.walmart.com/innovation/20180920/how-vr-is-transforming-the-way-we-train-associates},
  year         = {2018},
}

@misc{25,
  author       = {Lang, Ben},
  title        = {An Introduction to Positional Tracking and Degrees of Freedom (DOF)},
  URL          = {https://www.roadtovr.com/introduction-positional-tracking-degrees-freedom-dof/},
  year         = {2013},
}

@inproceedings{26,
  title={Virtuelles Training in der Krankenpflege: Erste Erfahrungen mit Ultra-mobilen Head-Mounted-Displays},
  author={Derksen, Melanie and Zhang, Le and Sch{\"a}fer, Marc and Schr{\"o}der, Dimitri and Pfeiffer, Thies},
  year = {2016}
}

@article{27,
  title={Virtual reality: how much immersion is enough?},
  author={Bowman, Doug A and McMahan, Ryan P},
  journal={Computer},
  volume={40},
  number={7},
  year={2007},
  publisher={IEEE}
}

@book{28,
 author = {Jerald, Jason},
 title = {The VR Book: Human-Centered Design for Virtual Reality},
 year = {2016},
 isbn = {978-1-97000-112-9},
 publisher = {Association for Computing Machinery and Morgan \&\#38; Claypool},
 address = {New York, NY, USA},
}

@article{29,
  title={Affective interactions using virtual reality: the link between presence and emotions},
  author={Riva, Giuseppe and Mantovani, Fabrizia and Capideville, Claret Samantha and Preziosa, Alessandra and Morganti, Francesca and Villani, Daniela and Gaggioli, Andrea and Botella, Cristina and Alca{\~n}iz, Mariano},
  journal={CyberPsychology \& Behavior},
  volume={10},
  number={1},
  pages={45--56},
  year={2007},
  publisher={Mary Ann Liebert, Inc. 2 Madison Avenue Larchmont, NY 10538 USA}
}

@Article{30,
author="Sch{\"o}ne, Benjamin
and Wessels, Marlene
and Gruber, Thomas",
title="Experiences in Virtual Reality: a Window to Autobiographical Memory",
journal="Current Psychology",
year="2017",
month="Jul",
day="20",
abstract="VR-based paradigms could substantially increase the ecological validity of various psychological research topics as VR allows for submerging into real-life experiences under controlled laboratory conditions. In particular, in the field of mnemonic research, concerns have been raised that ``laboratory memory'' differs significantly from ``real-life'' autobiographical memory. Our study aimed to assess the immersive qualities of VR not only upon application but -more importantly- during the retrieval of the virtual experiences subsequent to a VR session. We presented participants with either a 360{\textdegree} VR or a 2D video of a motorcycle ride followed by an unannounced recognition memory task 48 h later. Increased retrieval success and delayed reaction times in the VR group indicate that immersive VR experiences become part of an extensive autobiographical associative network, whereas the conventional video experience remains an isolated episodic event.",
issn="1936-4733",
doi="10.1007/s12144-017-9648-y",
url="https://doi.org/10.1007/s12144-017-9648-y"
}

@misc{31,
  URL={https://www.oculus.com/rift/#oui-csl-rift-games=star-trek},
  title={Official Site Oculus Rift},
  year={2018},
  author={Oculus}
}

@misc{32,
  URL={https://www.vive.com/us/},
  title={Official Site HTC Vive},
  year={2018},
  author={Vive}
}

@misc{33,
  URL={https://vr.google.com/cardboard/},
  title={Official Site Google Cardboard},
  year={2018},
  author={Google}
}

@misc{34,
  URL={https://www.samsung.com/global/galaxy/gear-vr/},
  title={Official Site Samsung Gear VR},
  year={2018},
  author={Samsung}
}

@misc{35,
  URL={https://vr.google.com/daydream/},
  title={Official Site Google Daydream VR},
  year={2018},
  author={Google}
}

@misc{36,
  URL={https://www.oculus.com/go/},
  title={Official Site Oculus Go},
  year={2018},
  author={Oculus}
}

@misc{37,
  
}

@article{39,
  abstract     = {Unsere Augen sind für die Wahrnehmung unserer Umwelt wichtig und geben gleichzeitig wertvolle Informationen über unsere Aufmerksamkeit und damit unsere Denkprozesse preis. Wir Menschen nutzen dies ganz natürlich in der alltäglichen Kommunikation. Mit einer echtzeitfähigen Blickbewegungsmessung ausgestattet können auch technische Systeme den Nutzern wichtige Informationen von den Augen ablesen. Der Artikel beschreibt verschiedene Ansätze wie in der Konstruktion, der Instruktion von Robotern oder der Medizin Blickbewegungen nutzbringend eingesetzt werden können. 
/
We use our eyes to perceive our everyday environment. In doing so, we also reveal our current focus of attention and thus allow others to draw insights regarding our internal cognition processes. We humans make use of this dual function of the eyes quite naturally in everyday communication. Using realtime eye tracking, technical systems can learn to read relevant information from their users' eyes. This article describes approaches to make use of gaze information in construction tasks, the instruction of robots or in medical applications.},
  author       = {Pfeiffer, Thies and Wachsmuth, Ipke},
  journal      = {at - Automatisierungstechnik},
  keyword      = {Visual Attention, Multimodal Interaction, Human-Machine Interaction, Eye Tracking, Attentive Interfaces, Visuelle Aufmerksamkeit, Multimodale Interaktion, Mensch-Maschine-Interaktion, Aufmerksame Benutzerschnittstellen, Blickbewegungsmessung, Gaze-based Interaction},
  number       = {11},
  pages        = {770--776},
  publisher    = {Walter de Gruyter GmbH},
  title        = {{Multimodale blickbasierte Interaktion}},
  doi          = {10.1524/auto.2013.1058},
  volume       = {61},
  year         = {2013},
}

@inproceedings{40,
  abstract     = {Das Paper arbeitet den Forschungsstand zur Überwindung von Höhenunterschieden in der Virtuellen Realität (VR) auf und diskutiert insbesondere deren Einsatz in egozentrischer Perspektive. Am konkreten Beispiel einer VR-Version des Computerspiels Minecraft wird herausgestellt, dass bestehende Ansätze den Anforderungen dieser Anwendungen nicht genügen.},
  author       = {Orlikowski, Matthias and Bongartz, Richard and Reddersen, Andrea and Reuter, Jana and Pfeiffer, Thies},
  booktitle    = {Virtuelle und Erweiterte Realität - 10. Workshop der GI-Fachgruppe VR/AR},
  editor       = {Latoschik, Marc Erich and Staadt, Oliver and Steinicke, Frank},
  keyword      = {Virtual Reality, Human-Computer Interaction},
  pages        = {193--196},
  publisher    = {Shaker Verlag},
  title        = {{Springen in der Virtuellen Realität: Analyse von Navigationsformen zur Überwindung von Höhenunterschieden am Beispiel von MinecraftVR}},
  year         = {2013},
}

@inproceedings{41,
  abstract     = {Die synthetische Stimulation der visuellen Wahrnehmung ist seit jeher im Fokus von Virtueller und Erweiterter Realität und die möglichst exakte Bestimmung der Nutzerperspektive auf die dreidimensionale Welt eine der Kernaufgaben. Bislang gibt es jedoch nur einige exemplarische Ansätze, in denen die Blickrichtung des Nutzers oder gar die Verteilung der visuellen Aufmerksamkeit im Raum genauer bestimmt wird. Macht man diese Informationen der Anwendungslogik verfügbar, könnten existierende Verfahren zur Visualisierung weiter optimiert und neue Verfahren entwickelt werden. Darüber hinaus erschließen sich damit Blicke als Interaktionsmodalität. Aufbauend auf langjährigen Erfahrungen mit der Blickinteraktion in der Virtuellen Realität beschreibt der Artikel Komponenten für einen Szenengraph, mit dem sich blickbasierte Interaktionen leicht und entlang gewohnter Prinzipien realisieren lassen.},
  author       = {Pfeiffer, Thies},
  booktitle    = {11. Paderborner Workshop Augmented and Virtual Reality in der Produktentstehung},
  editor       = {Gausemeier, Jürgen and Grafe, Michael and Meyer auf der Heide, Friedhelm},
  keyword      = {Virtual Reality, Human-Machine Interaction, Visual Attention, Gaze-based Interaction},
  pages        = {295--307},
  publisher    = {Heinz Nixdorf Institut, Universität Paderborn},
  title        = {{Visuelle Aufmerksamkeit in Virtueller und Erweiterter Realität: Integration und Nutzung im Szenengraphen}},
  volume       = {311},
  year         = {2013},
}

@inbook{42,
  abstract     = {Human hand gestures are very swift and difficult to observe from the (often) distant perspective of a scientific overhearer. Not uncommonly, fingers are occluded by other body parts or context objects and the true hand posture is often only revealed to the addressee. In addition to that, as the hand has many degrees of freedom and the annotation has to cover positions and orientations in a 3D world – which is less accessible from the typical computer-desktop workplace of an annotator than, let’s say, spoken language – the annotation of hand postures is quite expensive and complex.

Fortunately, the research on virtual reality technology has brought about data gloves in the first place, which were meant as an interaction device allowing humans to manipulate entities in a virtual world. Since its release, however, many different applications have been found. Data gloves are devices that track most of the joints of the human hand and generate data-sets describing the posture of the hand several times a second. The article reviews different types of data gloves, discusses representation formats and ways to support annotation, and presents best practices for study design using data gloves as recording devices.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Body-Language-Communication: An International Handbook on Multimodality in Human Interaction},
  editor       = {Müller, Cornelia and Cienki, Alan and Fricke, Ellen and Ladewig, Silva H. and McNeill, David and Teßendorf, Sedinha},
  keyword      = {Multimodal Communication, Multimodal Corpora, Motion Capturing, Data Gloves},
  pages        = {868--869},
  publisher    = {Mouton de Gruyter},
  title        = {{Documentation of gestures with data gloves}},
  doi          = {10.1515/9783110261318.868},
  volume       = {38/1},
  year         = {2013},
}

@inbook{43,
  abstract     = {For the scientific observation of non-verbal communication behavior, video recordings are the state of the art. However, everyone who has conducted at least one video-based
study has probably made the experience, that it is difficult to get the setup right, with respect to image resolution, illumination, perspective, occlusions, etc. And even more effort is needed for the annotation of the data. Frequently, even short interaction sequences may consume weeks or even months of rigorous full-time annotations.
One way to overcome some of these issues is the use of motion capturing for assessing (not only) communicative body movements. There are several competing tracking technologies available, each with its own benefits and drawbacks. The article provides an overview of the basic types of tracking systems, presents representation formats and tools for the analysis of motion data, provides pointers to some studies using motion capture and discusses best practices for study design. However, the article also stresses that motion capturing still requires some expertise and is only starting to become mobile
and reasonably priced – arguments not to be neglected.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Handbücher zur Sprach- und Kommunikationswissenschaft / Handbooks of Linguistics and Communication Science},
  editor       = {Müller, Cornelia and Cienki, Alan and Fricke, Ellen and Ladewig, Silva H. and McNeill, David and Teßendorf, Sedinha},
  keyword      = {Multimodal Communication, Motion Capturing, Gesture Annotation, Multimodal Corpora},
  pages        = {857--868},
  publisher    = {Mouton de Gruyter},
  title        = {{Documentation of gestures with motion capture}},
  doi          = {10.1515/9783110261318.857},
  volume       = {38/1},
  year         = {2013},
}

@misc{44,
  author       = {Pfeiffer-Leßmann, Nadine and Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {Book of Abstracts of the 17th European Conference on Eye Movements},
  editor       = {Holmqvist, Kenneth and Mulvey, F. and Johansson, Roger},
  keyword      = {Joint Attention},
  location     = {Lund, Sweden},
  number       = {3},
  pages        = {152--152},
  publisher    = {Journal of Eye Movement Research},
  title        = {{A model of joint attention for humans and machines}},
  volume       = {6},
  year         = {2013},
}

@inproceedings{45,
  abstract     = {A fundamental problem in manual based gesture semantics reconstruction is the specification of preferred semantic concepts for gesture trajectories. This issue is complicated by problems human raters have annotating fast-paced three dimensional trajectories. Based on a detailed example of a gesticulated circular trajectory,
we present a data-driven approach that covers parts of the semantic reconstruction by making use of motion capturing
(mocap) technology. In our FA3ME framework we use a complex event processing approach to analyse and annotate multi-modal events. This framework provides grounds for a detailed description of how to get at the semantic concept of circularity observed in the data.},
  author       = {Pfeiffer, Thies and Hofmann, Florian and Hahn, Florian and Rieser, Hannes and Röpke, Insa},
  booktitle    = {Proceedings of the Special Interest Group on Discourse and Dialog (SIGDIAL) 2013 Conference},
  editor       = {Eskenazi, Maxine and Strube, Michael and Di Eugenio, Barbara and Williams, Jason D.},
  keyword      = {Multimodal Communication},
  location     = {Metz, France},
  pages        = {270--279},
  publisher    = {Association for Computational Linguistics},
  title        = {{Gesture semantics reconstruction based on motion capturing and complex event processing: a circular shape example}},
  year         = {2013},
}

@inproceedings{46,
  abstract     = {This paper presents ongoing work on the design, deployment and evaluation of a multimodal data acquisition architecture
which utilises minimally invasive motion, head, eye and gaze tracking alongside high-quality audiovisual recording of
human interactions. The different data streams are centrally collected and visualised at a single point and in real time by
means of integration in a virtual reality (VR) environment. The overall aim of this endeavour is the implementation of a
multimodal data acquisition facility for the purpose of studying non-verbal phenomena such as feedback gestures,
hand and pointing gestures and multi-modal alignment. In the first part of this work that is described here, a series of tests
were performed in order to evaluate the feasibility of tracking feedback head gestures using the proposed architecture.},
  author       = {Kousidis, Spyridon and Pfeiffer, Thies and Malisz, Zofia and Wagner, Petra and Schlangen, David},
  booktitle    = {Proceedings of the Interdisciplinary Workshop on Feedback Behaviors in Dialog, INTERSPEECH2012 Satellite Workshop},
  keyword      = {Multimodal Communication},
  location     = {Stevenson, WA},
  pages        = {39--42},
  title        = {{Evaluating a minimally invasive laboratory architecture for recording multimodal conversational data.}},
  year         = {2012},
}

@misc{47,
  author       = {Pfeiffer-Leßmann, Nadine and Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {Proceedings of KogWis 2012},
  editor       = {Dörner, Dietrich and Goebel, Rainer and Oaksford, Mike and Pauen, Michael and Stern, Elsbeth},
  keyword      = {gaze-based interaction, cognitive modeling, joint attention},
  location     = {Bamberg, Germany},
  pages        = {96--97},
  publisher    = {University of Bamberg Press},
  title        = {{An operational model of joint attention - Timing of the initiate-act in interactions with a virtual human}},
  year         = {2012},
}

@inproceedings{48,
  author       = {Pfeiffer-Leßmann, Nadine and Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {Proceedings of the 34th Annual Conference of the Cognitive Science Society},
  editor       = {Miyake, Naomi  and Peebles, David  and Cooper, Richard P. },
  keyword      = {joint attention, Gaze-based Interaction, social interaction, virtual humans},
  location     = {Sapporo, Japan},
  pages        = {851--856},
  publisher    = {Cognitive Science Society},
  title        = {{An operational model of joint attention - timing of gaze patterns in interactions between humans and a virtual human}},
  year         = {2012},
}

@inproceedings{49,
  abstract     = {Eye tracking and hand motion (or mouse) tracking are complementary techniques to study the dynamics underlying
human cognition. Eye tracking provides information about attention, reasoning, mental imagery, but figuring out the dynamics of cognition is hard. On the other hand, hand movement reveals the hidden states of high-level cognition as a continuous trajectory, but the detailed process is difficult to infer. Here, we use both eye and hand tracking while the subject watches a video drama and plays a multimodal memory game (MMG), a memory recall task designed to investigate the mechanism of recalling the contents of dramas. Our experimental results show that eye tracking and mouse tacking provide complementary information on cognitive processes. In particular, we found that, when humans make difficult decisions, they tend to ask ’Is the
distractor wrong?’, rather than ’Is the decision right?’.},
  author       = {Kim, Eun-Sol and Kim, Jiseob and Pfeiffer, Thies and Wachsmuth, Ipke and Zhang, Byoung-Tak},
  booktitle    = {Proceedings of the 34th Annual Meeting of the Cognitive Science Society},
  editor       = {Miyake, Naomi and Peebles, David and Cooper, Richard P.},
  keyword      = {Gaze-based Interaction},
  pages        = {2723},
  title        = {{‘Is this right?’ or ‘Is that wrong?’: Evidence from dynamic eye-hand movement in decision making [Abstract]}},
  year         = {2012},
}

@inproceedings{50,
  abstract     = {In this paper, we argue that empirical research on genuine linguistic topics, such as on the production of multimodal utterances in the speaker and the interpretation of the multimodal signals in the interlocutor, can greatly benefit from the use of virtual reality technologies. Established methodologies for research on multimodal interactions, such as the presentation of pre-recorded 2D videos of interaction partners as stimuli and the recording of interaction partners using multiple 2D video cameras have crucial shortcomings regarding ecological validity and the precision of measurements that can be achieved. In addition, these methodologies enforce restrictions on the researcher. The stimuli, for example, are not very interactive and thus not as close to natural interactions as ultimately desired. Also, the analysis of 2D video recordings requires intensive manual annotations, often frame-by-frame, which negatively affects the feasible number of interactions which can be included in a study. The technologies bundled under the term virtual reality offer exciting possibilities for the linguistic researcher: gestures can be tracked without being restricted to fixed perspectives, annotation can be done on large corpora (semi-)automatically and virtual characters can be used to produce specific linguistic stimuli in a repetitive but interactive fashion. Moreover, immersive 3D visualizations can be used to recreate a simulation of the recorded interactions by fusing the raw data with theoretic models to support an iterative data-driven development of linguistic theories. This paper discusses the potential of virtual reality technologies for linguistic research and provides examples for the application of the methodology.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Virtual Reality Short Papers and Posters (VRW)},
  editor       = {Coquillart, Sabine and Feiner, Steven and Kiyokawa, Kiyoshi},
  keyword      = {Linguistics, Motion Capturing, Intelligent Virtual Agents, Multimodal Communication, Virtual Reality},
  location     = {Costa Mesa, CA, USA},
  pages        = {83--84},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
  title        = {{Using virtual reality technology in linguistic research}},
  doi          = {10.1109/VR.2012.6180893},
  year         = {2012},
}

@inproceedings{51,
  abstract     = {Modern computer-algebra programs are able to solve a wide
range of mathematical calculations. However, they are not able to understand and solve math text problems in which the equation is described in terms of natural language instead of mathematical formulas. Interestingly, there are only few known approaches to solve math word problems algorithmically and most of employ models based on frames. To overcome problems with existing models, we propose a model based on augmented semantic networks to represent the mathematical structure behind word problems. This model is implemented in our Solver for Mathematical Text Problems (SoMaTePs) [1], where the math problem is extracted via natural language processing, transformed in mathematical equations and solved by a state-of-the-art computer-algebra program. SoMaTePs is able to understand and solve mathematical text problems from German primary school books and could be extended to other languages by exchanging the language model in the natural language processing module.},
  author       = {Liguda, Christian and Pfeiffer, Thies},
  booktitle    = {Natural Language Processing and Information Systems/17th International Conference on Applications of Natural Language to Information Systems},
  editor       = {Bouma, Gosse and Ittoo, Ashwin and Métais, Elisabeth and Wortmann, Hans},
  keyword      = {Artificial Intelligence},
  location     = {Groningen, Netherlands},
  pages        = {247--252},
  publisher    = {Springer},
  title        = {{Modeling math word problems with augmented semantic networks}},
  doi          = {10.1007/978-3-642-31178-9_29},
  volume       = {7337},
  year         = {2012},
}

@inproceedings{52,
  abstract     = {Knowledge about the point of regard is a major key for the analysis of visual attention in areas such as psycholinguistics, psychology, neurobiology, computer science and human factors. Eye tracking is thus an established methodology in these areas, e.g. for investigating search processes, human communication behavior, product design or human-computer interaction. As eye tracking is a process which depends heavily on technology, the progress of gaze use in these scientific areas is tied to the advancements of eye-tracking technology. It is thus not surprising that in the last decades, research was primarily based on 2D stimuli and rather static scenarios, regarding both content and observer. Only with the advancements in mobile and robust eye-tracking systems, the observer is freed to physically interact in a 3D target scenario. Measuring and analyzing the point of regards in 3D space, however, requires additional techniques for data acquisition and scientific visualization. We describe the process for measuring the 3D point of regard and provide our own implementation of this process, which extends recent approaches of combining eye tracking with motion capturing, including holistic estimations of the 3D point of regard. In addition, we present a refined version of 3D attention volumes for representing and visualizing attention in 3D space.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Proceedings of the Symposium on Eye Tracking Research and Applications},
  editor       = {Spencer, Stephen N.},
  keyword      = {gaze tracking, visualization, motion tracking, Gaze-based Interaction, visual attention, 3d},
  location     = {Santa Barbara, CA, USA},
  pages        = {29--36},
  publisher    = {Association for Computing Machinery (ACM)},
  title        = {{Measuring and visualizing attention in space with 3D attention volumes}},
  doi          = {10.1145/2168556.2168560},
  year         = {2012},
}

@inproceedings{53,
  abstract     = {Referring to objects using multimodal deictic expressions is an important form of communication. This work addresses the question on how pragmatic factors affect content distribution between the modalities speech and gesture. This is done by analyzing a study on deictic pointing gestures to objects under two conditions: with and without speech. The relevant pragmatic factor was the distance to the referent object. As one main result two strategies were identified which were used by participants to adapt their gestures to the condition. This knowledge can be used, e.g., to improve the naturalness of pointing gestures employed by embodied conversational agents.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Gestures and Sign Language in Human-Computer Interaction and Embodied Communication, 9th International Gesture Workshop, GW 2011},
  editor       = {Efthimiou, Eleni and Kouroupetroglou, Georgios and Fotinea, Stavroula-Evita},
  keyword      = {Multimodal Communication},
  location     = {Athens, Greece},
  pages        = {238--249},
  publisher    = {Springer-Verlag GmbH},
  title        = {{Interaction between Speech and Gesture: Strategies for Pointing to Distant Objects}},
  doi          = {10.1007/978-3-642-34182-3_22},
  year         = {2012},
}

@inproceedings{54,
  abstract     = {The time course and the distribution of visual attention are powerful measures for the evaluation of the usability of products. Eye tracking is thus an established method for evaluating websites, software ergonomy or modern cockpits for cars or airplanes. In most cases, however, the point of regard is measured on 2D products. This article presents work that uses an approach to measure the point of regard in 3D to generate 3D Attention Volumes as a qualitative 3D visualization of the distribution of visual attention. This visualization can be used to evaluate the design of virtual products in an immersive 3D setting, similar as heatmaps are used to assess the design of websites.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Proceedings of the IEEE Virtual Reality 2012},
  keyword      = {Gaze-based Interaction},
  location     = {Orange County, CA, USA},
  pages        = {117--118},
  publisher    = {IEEE},
  title        = {{3D Attention Volumes for Usability Studies in Virtual Reality}},
  year         = {2012},
}

@inproceedings{55,
  abstract     = {Swiftness and robustness of natural communication is tied to the redundancy and complementarity found in our multimodal communication. Swiftness and robustness of human-computer interaction (HCI) is also a key to the success of a virtual reality (VR) environment. The interpretation of multimodal interaction signals has therefore been considered a high goal in VR research, e.g. following the visions of Bolt's put-that-there in 1980. It is our impression that research on user interfaces for VR systems has been focused primarily on finding and evaluating technical solutions and thus followed a technology-oriented approach to HCI. In this article, we argue to complement this by a human-oriented approach based on the observation of human-human interaction. The aim is to find models of human-human interaction that can be used to create user interfaces that feel natural. As the field of Linguistics is dedicated to the observation and modeling of human-human communication, it could be worthwhile to approach natural user interfaces from a linguistic perspective. We expect at least two benefits from following this approach. First, the human-oriented approach substantiates our understanding of natural human interactions. Second, it brings about a new perspective by taking the interaction capabilities of a human addressee into account, which are not often explicitly considered or compared with that of the system. As a consequence of following both approaches to create user interfaces, we expect more general models of human interaction to emerge.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Proceedings of the IEEE Virtual Reality 2012},
  keyword      = {Linguistics, Virtual Reality, Human-Computer Interaction, Deixis, Multimodal Communication},
  location     = {Orange County, CA, USA},
  pages        = {89--90},
  publisher    = {IEEE},
  title        = {{Towards a Linguistically Motivated Model for Selection in Virtual Reality}},
  year         = {2012},
}

@inbook{56,
  author       = {Lücking, Andy and Pfeiffer, Thies},
  booktitle    = {Handbook of Technical Communication},
  editor       = {Mehler, Alexander and Romary, Laurent},
  keyword      = {Multimodal Communication},
  pages        = {591--644},
  publisher    = {Mouton de Gruyter},
  title        = {{Framing Multimodal Technical Communication. With Focal Points in Speech-Gesture-Integration and Gaze Recognition}},
  doi          = {10.1515/9783110224948.591},
  volume       = {8},
  year         = {2012},
}

@inproceedings{57,
  abstract     = {The idea of using gaze as an interaction modality has been put forward by the famous work of Bolt in 1981. In virtual reality (VR), gaze has been used for several means since then: view-dependent optimization of rendering, intelligent information visualization, reference communication in distributed telecommunication settings and object selection. 
Our own research aims at improving gaze-based interaction methods in general. In this paper, gaze-based interaction is examined in a fast-paced selection task to identify current usability problems of gaze-based interaction and to develop best practices. To this end, an immersive Asteroids-like shooter called Eyesteroids was developed to support a study comparing manual and gaze-based interaction methods. Criteria for the evaluation were interaction performance and user immersion. The results indicate that while both modalities (hand and gaze) work well for the task, manual interaction is easier to use and often more accurate than the implemented gaze-based methods. The reasons are discussed and the best practices as well as options for further improvements of gaze-based interaction methods are presented.},
  author       = {Hülsmann, Felix and Dankert, Timo and Pfeiffer, Thies},
  booktitle    = {Proceedings of the Workshop Virtuelle \& Erweiterte Realität 2011},
  editor       = {Bohn, Christian-A. and Mostafawy, Sina},
  keyword      = {Gaze-based Interaction},
  pages        = {1--12},
  publisher    = {Shaker Verlag},
  title        = {{Comparing gaze-based and manual interaction in a fast-paced gaming task in Virtual Reality}},
  year         = {2011},
}

@inproceedings{58,
  abstract     = {The idea of using gaze as an interaction modality has been put forward by the famous work of Bolt in 1981. In virtual reality (VR), gaze has been used for several means since then: view-dependent optimization of rendering, intelligent information visualization, reference communication in distributed telecommunication settings and object selection. 
Our own research aims at improving gaze-based interaction methods in general. In this paper, gaze-based interaction is examined in a fast-paced selection task to identify current usability problems of gaze-based interaction and to develop best practices. To this end, an immersive Asteroids-like shooter called Eyesteroids was developed to support a study comparing manual and gaze-based interaction methods. Criteria for the evaluation were interaction performance and user immersion. The results indicate that while both modalities (hand and gaze) work well for the task, manual interaction is easier to use and often more accurate than the implemented gaze-based methods. The reasons are discussed and the best practices as well as options for further improvements of gaze-based interaction methods are presented.},
  author       = {Renner, Patrick and Lüdike, Nico and Wittrowski, Jens and Pfeiffer, Thies},
  booktitle    = {Proceedings of the Workshop Virtuelle \& Erweiterte Realität 2011},
  editor       = {Bohn, Christian-A. and Mostafawy, Sina },
  keyword      = {Virtual Reality, Gaze-based Interaction},
  pages        = {13--24},
  publisher    = {Shaker Verlag},
  title        = {{Towards Continuous Gaze-Based Interaction in 3D Environments - Unobtrusive Calibration and Accuracy Monitoring}},
  year         = {2011},
}

@misc{59,
  author       = {Essig, Kai and Pfeiffer, Thies and Sand, Norbert and Künsemöller, Jörn and Ritter, Helge and Schack, Thomas},
  booktitle    = {Journal of Eye Movement Research},
  keyword      = {annotation, object recognition, computer vision, Gaze-based Interaction, eye-tracking},
  location     = {Marseille},
  number       = {3},
  pages        = {48},
  title        = {{JVideoGazer - Towards an Automatic Annotation of Gaze Videos from Natural Scenes}},
  volume       = {4},
  year         = {2011},
}

@article{60,
  abstract     = {Social networking platforms (SNPs) are meant to reflect the social relationships of their users. Users typically enter very personal information and should get useful feedback about their social network. They should indeed be empowered to exploit the information they have entered. In reality, however, most SNPs actually hide the structure of the user’s rich social network behind very restricted text-based user interfaces and large parts of the potential information which could be extracted from the entered data lies fallow. This article presents results from a user study showing that 3D visualizations of social graphs can be utilized more effectively – and moreover – are preferred by users compared to traditional text-based interfaces. Subsequently, the article addresses the problem of how to deploy interactive 3D graphical interfaces to large user communities. This is demonstrated on the social graph app-
lication FriendGraph3D for Facebook.},
  author       = {Mattar, Nikita and Pfeiffer, Thies},
  journal      = {International Journal of Computer Information Systems and Industrial Management Applications},
  keyword      = {information visualization, interactive graphs, social networks, web technology, 3d graphics},
  pages        = {427--434},
  title        = {{Interactive 3D graphs for web-based social networking platforms}},
  volume       = {3},
  year         = {2011},
}

@inproceedings{61,
  abstract     = {Since 2004 the virtual agent Max is living at the Heinz Nixdorf MuseumsForum – a computer science museum. He is welcoming and entertaining visitors ten hours a day, six days a week, for seven years. This article brings together the experiences made by the staff of the museum, the scientists who created and maintained the installation, the visitors and the agent himself. It provides insights about the installation’s hard- and software and presents highlights of the agent’s ontogenesis in terms of the features he has gained. A special focus is on the means Max uses to engage with visitors and the features which make him attractive.},
  author       = {Pfeiffer, Thies and Liguda, Christian and Wachsmuth, Ipke and Stein, Stefan},
  booktitle    = {Proceedings of the Re-Thinking Technology in Museums 2011 - Emerging Experiences},
  editor       = {Barbieri , Sara  and Scott, Katherine  and Ciolfi, Luigina},
  keyword      = {Embodied Conversational Agent, ECA, Chatterbot, Max, Museum, Artificial Intelligence, Virtual Agent},
  location     = {Limerick},
  pages        = {121--131},
  publisher    = {thinkk creative \& the University of Limerick},
  title        = {{Living with a Virtual Agent: Seven Years with an Embodied Conversational Agent at the Heinz Nixdorf MuseumsForum}},
  year         = {2011},
}

@inproceedings{62,
  abstract     = {Die Messung visueller Aufmerksamkeit mittels Eye-Tracking ist eine etablierte Methode in der Bewertung von Ergonomie und Usability. Ihr Gegenstandsbereich beschränkt sich jedoch primär auf 2D-Inhalte wie Webseiten, Produktfotos oder –videos. Bewegte Interaktion im dreidimensionalen Raum wird selten erfasst, weder am realen Objekt, noch am virtuellen Prototyp. Mit einer Aufmerksamkeitsmessung im Raum könnte der Gegenstandsbereich um diese Fälle deutlich erweitert werden. Der vorliegende Artikel arbeitet den aktuellen Stand der Forschung zur Messung visueller Aufmerksamkeit im Raum auf. Dabei werden insbesondere die zu bewältigenden Schwierigkeiten herausgearbeitet und Lösungsansätze aufgezeigt. Als Schwerpunkt werden drei Themen an eigenen Arbeiten diskutiert: Aufbau und Kalibrierung der Systeme, Bestimmung des betrachteten Volumens und Visualisierung der Aufmerksamkeit im Raum.},
  author       = {Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {10. Paderborner Workshop Augmented and Virtual Reality in der Produktentstehung},
  editor       = {Gausemeier, Jürgen and Grafe, Michael and Meyer auf der Heide, Friedhelm},
  keyword      = {Gaze-based Interaction},
  number       = {295},
  pages        = {39--51},
  publisher    = {Heinz Nixdorf Institut, Universität Paderborn},
  title        = {{Dreidimensionale Erfassung visueller Aufmerksamkeit für Usability-Bewertungen an virtuellen Prototypen}},
  year         = {2011},
}

@book{63,
  abstract     = {When humans communicate, we use deictic expressions to refer to objects in our surrounding and put them in the context of our actions. In face to face interaction, we can complement verbal expressions with gestures and, hence, we do not need to be too precise in our verbal protocols. Our interlocutors hear our speaking; see our gestures and they even read our eyes. They interpret our deictic expressions, try to identify the referents and – normally – they will understand. If only machines could do alike.

The driving vision behind the research in this thesis are multimodal conversational interfaces where humans are engaged in natural dialogues with computer systems. The embodied conversational agent Max developed in the A.I. group at Bielefeld University is an example of such an interface. Max is already able to produce multimodal deictic expressions using speech, gaze and gestures, but his capabilities to understand humans are not on par. If he was able to resolve multimodal deictic expressions, his understanding of humans would increase and interacting with him would become more natural. 

Following this vision, we as scientists are confronted with several challenges. First, accurate models for human pointing have to be found. Second, precise data on multimodal interactions has to be collected, integrated and analyzed in order to create these models. This data is multimodal (transcripts, voice and video recordings, annotations) and not directly accessible for analysis (voice and video recordings). Third, technologies have to be developed to support the integration and the analysis of the multimodal data. Fourth, the created models have to be implemented, evaluated and optimized until they allow a natural interaction with the conversational interface.

To this ends, this work aims to deepen our knowledge of human non-verbal deixis, specifically of manual and gaze pointing, and to apply this knowledge in conversational interfaces. At the core of the theoretical and empirical investigations of this thesis are models for the interpretation of pointing gestures to objects. These models address the following questions: When are we pointing? Where are we pointing to? Which objects are we pointing at? With respect to these questions, this thesis makes the following three contributions: First, gaze-based interaction technology for 3D environments: Gaze plays an important role in human communication, not only in deictic reference. Yet, technology for gaze interaction is still less developed than technology for manual interaction.

In this thesis, we have developed components for real-time tracking of eye movements and of the point of regard in 3D space and integrated them in a framework for Deictic Reference In Virtual Environments (DRIVE). DRIVE provides viable information about human communicative behavior in real-time. This data can be used to investigate and to design processes on higher cognitive levels, such as turn-taking, check-backs, shared attention and resolving deictic reference. 

Second, data-driven modeling: We answer the theoretical questions about timing, direction, accuracy and dereferential power of pointing by data-driven modeling.

As empirical basis for the simulations, we created a substantial corpus with highprecision data from an extensive study on multimodal pointing. Two further studies complemented this effort with substantial data on gaze pointing in 3D. Based on this data, we have developed several models of pointing and successfully created a model for the interpretation of manual pointing that achieves a human-like performance level.

Third, new methodologies for research on multimodal deixis in the fields of linguistics and computer science: The experimental-simulative approach to modeling – which we follow in this thesis – requires large collections of heterogeneous data to be recorded, integrated, analyzed and resimulated. To support the researcher in these tasks, we developed the Interactive Augmented Data Explorer (IADE). IADE is an innovative tool for research on multimodal interaction based on virtual reality technology. It allows researchers to literally immerse into multimodal data
and interactively explore them in real-time and in virtual space. With IADE we have also extended established approaches for scientific visualization of linguistic data to 3D, which previously existed only for 2D methods of analysis (e.g. video recordings or computer screen experiments). By this means, we extended Mc-Neill’s 2D depiction of the gesture space to gesture space volumes expanding in time and space. Similarly, we created attention volumes, a new way to visualize the distribution of attention in 3D environments.},
  author       = {Pfeiffer, Thies},
  keyword      = {Multimodal Communication, Gaze-based Interaction},
  pages        = {217},
  publisher    = {Shaker Verlag},
  title        = {{Understanding Multimodal Deixis with Gaze and Gesture in Conversational Interfaces}},
  year         = {2011},
}

@inproceedings{64,
  abstract     = {Referring to objects using multimodal deictic expressions is an important form of communication. This work addresses the question on how content is distributed between the modalities speech and gesture by comparing deictic pointing gestures to objects with and without speech. As a result, two main strategies used by participants to adapt their gestures to the condition were identified. This knowledge can be used, e.g., to improve the naturalness of pointing gestures employed by embodied conversational agents.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Gestures in Embodied Communication and Human-Computer Interaction, 9th International Gesture Workshop, GW 2011},
  editor       = {Efthimiou, Eleni and Kouroupetroglou, Georgios},
  keyword      = {Interaction between Speech and Gesture, Gesture, Speech, Pointing, Multimodal Fusion, Multimodal Communication},
  location     = {Athens, Greece},
  pages        = {109--112},
  publisher    = {National and Kapodistrian University of Athens},
  title        = {{Interaction between Speech and Gesture: Strategies for Pointing to Distant Objects}},
  year         = {2011},
}

@inproceedings{65,
  abstract     = {Solving word problems is an important part in school education in primary as well as in high school. Although, the equations that are given by a word problem could be solved by most computer algebra programs without problems, there are just few systems that are able to solve word problems. In this paper we present the ongoing work on a system, that is able to solve word problems from german primary school math books.},
  author       = {Liguda, Christian and Pfeiffer, Thies},
  booktitle    = {First International Workshop on Algorithmic Intelligence},
  editor       = {Messerschmidt, Hartmut},
  keyword      = {Artificial Intelligence, Natural Language Processing, Math Word Problems},
  location     = {Berlin},
  title        = {{A Question Answer System for Math Word Problems}},
  year         = {2011},
}

@inproceedings{66,
  abstract     = {Classic techniques for navigation and selection such as Image-Plane and World in Miniature have been around for more than 20 years. In the course of a seminar on interaction in virtual reality we reconsidered five methods for navigation and two for selection. These methods were significantly extended by the use of up-to-date hardware such as Fingertracking devices and the Nintendo Wii Balance Board and evaluated in a virtual supermarket scenario. Two user studies, one on experts and one on novices, revealed information on usability and efficiency. As an outcome, the combination of Ray-Casting and Walking in Place turned out to be the fastest.},
  author       = {Renner, Patrick and Dankert, Timo and Schneider, Dorothe and Mattar, Nikita and Pfeiffer, Thies},
  booktitle    = {Virtuelle und Erweiterte Realitaet: 7. Workshop der GI-Fachgruppe VR/AR},
  keyword      = {human-machine-interaction
},
  location     = {Stuttgart},
  pages        = {71--82},
  publisher    = {Shaker Verlag},
  title        = {{Navigating and selecting in the virtual supermarket: review and update of classic interaction techniques}},
  year         = {2010},
}

@inproceedings{67,
  abstract     = {Humans perceive, reason and act within a 3D environment. In empirical methods, however, researchers often restrict themselves to 2D, either in using 2D content or relying on 2D recordings for analysis, such as videos or 2D eye movements. Regarding, e.g., multimodal deixis, we address the open question of the morphology of the referential space (Butterworth and Itakura, 2000), For modeling the referential space of gaze pointing, precise knowledge about the target of our participants’ visual attention is crucial. To this ends, we developed methods to assess the location of the point of regard, which are outlined here.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Proceedings of the KogWis 2010},
  editor       = {Haack, Johannes and Wiese, Heike and Abraham, Andreas and Chiarcos, Christian},
  keyword      = {Gaze-based Interaction},
  location     = {Potsdam, Germany},
  pages        = {220--221},
  publisher    = {Universitätsverlag Potsdam},
  title        = {{Tracking and Visualizing Visual Attention in Real 3D Space}},
  year         = {2010},
}

@inproceedings{68,
  abstract     = {From the perspective of individual users, social networking platforms (SNPs) are meant to reflect their social relationships. SNPs should provide feedback allowing users to exploit the information they have entered. In reality, however, most SNPs actually hide the rich social network constructed by the users in their databases behind simple user interfaces. These interfaces reduce the complexity of a user's social network to a text-based list in HTML. This article presents results from a user study showing that 3D visualizations of social graphs can be utilized more effectively – and moreover – are preferred by users compared to traditional text-based interfaces. Subsequently, the article addresses the problem of deployment of rich interfaces. A social graph application for Facebook is presented, demonstrating how WebGL and HTML5/X3D can be used to implement rich social applications based on upcoming web standards.},
  author       = {Mattar, Nikita and Pfeiffer, Thies},
  booktitle    = {Proceedings of the IADIS International Conference Web Virtual Reality and Three-Dimensional Worlds 2010},
  keyword      = {social networks},
  pages        = {269--276},
  publisher    = {IADIS Press},
  title        = {{Relationships in social networks revealed: a facebook app for social graphs in 3D based on X3DOM and WebGL}},
  year         = {2010},
}

@phdthesis{69,
  abstract     = {When humans communicate, we use deictic expressions to refer to objects in our surrounding and put them in the context of our actions. In face to face interaction, we can complement verbal expressions with gestures and, hence, we do not need to be too precise in our verbal protocols. Our interlocutors hear our speaking; see our gestures and they even read our eyes. They interpret our deictic expressions, try to identify the referents and -- normally -- they will understand. If only machines could do alike.
The driving vision behind the research in this thesis are multimodal conversational interfaces where humans are engaged in natural dialogues with computer systems. The embodied conversational agent Max developed in the A.I. group at Bielefeld University is an example of such an interface. Max is already able to produce multimodal deictic expressions using speech, gaze and gestures, but his capabilities to understand humans are not on par. If he was able to resolve multimodal deictic expressions, his understanding of humans would increase and interacting with him would become more natural.
Following this vision, we as scientists are confronted with several challenges. First, accurate models for human pointing have to be found. Second, precise data on multimodal interactions has to be collected, integrated and analyzed in order to create these models. This data is multimodal (transcripts, voice and video recordings, annotations) and not directly accessible for analysis (voice and video recordings). Third, technologies have to be developed to support the integration and the analysis of the multimodal data. Fourth, the created models have to be implemented, evaluated and optimized until they allow a natural interaction with the conversational interface.
To this ends, this work aims to deepen our knowledge of human non-verbal deixis, specifically of manual and gaze pointing, and to apply this knowledge in conversational interfaces. At the core of the theoretical and empirical investigations of this thesis are models for the interpretation of pointing gestures to objects. These models address the following questions: When are we pointing? Where are we pointing to? Which objects are we pointing at? With respect to these questions, this thesis makes the following three contributions:
First, gaze-based interaction technology for 3D environments: Gaze plays an important role in human communication, not only in deictic reference. Yet, technology for gaze interaction is still less developed than technology for manual interaction. In this thesis, we have developed components for real-time tracking of eye movements and of the point of regard in 3D space and integrated them in a framework for DRIVE. DRIVE provides viable information about human communicative behavior in real-time. This data can be used to investigate and to design processes on higher cognitive levels, such as turn-taking, check- backs, shared attention and resolving deictic reference.
Second, data-driven modeling: We answer the theoretical questions about timing, direction, accuracy and dereferential power of pointing by data-driven modeling. As empirical basis for the simulations, we created a substantial corpus with high-precision data from an extensive study on multimodal pointing. Two further studies complemented this effort with substantial data on gaze pointing in 3D. Based on this data, we have developed several models of pointing and successfully created a model for the interpretation of manual pointing that achieves a human-like performance level.
Third, new methodologies for research on multimodal deixis in the fields of linguistics and computer science: The experimental-simulative approach to modeling -- which we follow in this thesis -- requires large collections of heterogeneous data to be recorded, integrated, analyzed and resimulated. To support the researcher in these tasks, we developed the Interactive Augmented Data Explorer. IADE is an innovative tool for research on multimodal interaction based on virtual reality technology. It allows researchers to literally immerse into multimodal data and interactively explore them in real-time and in virtual space. With IADE we have also extended established approaches for scientific visualization of linguistic data to 3D, which previously existed only for 2D methods of analysis (e.g. video recordings or computer screen experiments). By this means, we extended McNeill's 2D depiction of the gesture space to gesture space volumes expanding in time and space. Similarly, we created attention volumes, a new way to visualize the distribution of attention in 3D environments.},
  author       = {Pfeiffer, Thies},
  keyword      = {Reference, Gesture, Deixis, Human-Computer Interaction, Mensch-Maschine-Schnittstelle, Lokale Deixis, Blickbewegung, Gaze, Virtuelle Realität, Multimodales System, Referenz <Linguistik>, Gestik, Multimodal Communication, Gaze-based Interaction},
  pages        = {241},
  publisher    = {Universitätsbibliothek},
  title        = {{Understanding multimodal deixis with gaze and gesture in conversational interfaces}},
  year         = {2010},
}

@inproceedings{70,
  abstract     = {Object deixis is at the core of language and an ideal example of multimodality. Speech, gaze and manual gestures are used by interlocutors to refer to objects in their 3D environment. The interplay of verbal expressions and gestures during deixis is an active research topic in linguistics as well as in human-computer interaction. Previously, we conducted a study on manual pointing during dialogue games using state-of-the art tracking technologies to record gestures with high spatial precision (Kranstedt, Lücking, Pfeiffer, Rieser and Wachsmuth, 2006), To reveal strategies in manual pointing gestures, we present an analysis of this data with a new visualization technique.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Proceedings of the KogWis 2010},
  editor       = {Haack, Johannes and Wiese, Heike and Abraham, Andreas and Chiarcos, Christian},
  keyword      = {Multimodal Communication},
  location     = {Potsdam},
  pages        = {221--222},
  publisher    = {Universitätsverlag Potsdam},
  title        = {{Object Deixis: Interaction Between Verbal Expressions and Manual Pointing Gestures}},
  year         = {2010},
}

@inproceedings{71,
  abstract     = {Die Portale für soziale Netzwerke im Internet gehen mittlerweile deutlich über die Verwaltung einfacher Bekanntschaftsbeziehungen hinaus. Ihnen liegen immer reichhaltigere Datenmodelle zu Grunde. Darstellung und Exploration dieser Netzwerke sind eine grosse Herausforderung für die Entwickler, wenn beides nicht zu einer solchen für die Benutzer werden soll. Im Rahmen eines studentischen Projektes wurde die dritte Dimension für die Darstellung des komplexen sozialen Netzwerkes von Last.fm nutzbar gemacht. Durch die entwickelte Anwendung SoNAR wird das Netzwerk interaktiv und intuitiv sowohl am Desktop, als auch in der Immersion einer dreiseitigen CAVE explorierbar. Unterschiedliche Relationen des sozialen Graphen können parallel exploriert und damit Zusammenhänge zwischen Individuen intuitiv erfahren werden. Eine Suchfunktion erlaubt dabei die fexible Komposition verschiedener Startknoten für die Exploration.},
  author       = {Bluhm, Andreas and Eickmeyer, Jens and Feith, Tobias and Mattar, Nikita and Pfeiffer, Thies},
  booktitle    = {Virtuelle und Erweiterte Realität: 6. Workshop der GI-Fachgruppe VR/AR},
  keyword      = {social networks},
  pages        = {269--280},
  publisher    = {Shaker Verlag},
  title        = {{Exploration von sozialen Netzwerken im 3D Raum am Beispiel von SONAR für Last.fm}},
  year         = {2009},
}

@inproceedings{72,
  abstract     = {The "Where?" is quite important for Mixed Reality applications: Where is the user looking at? Where should augmentations be displayed? The location of the overt visual attention of the user can be used both to disambiguate referent objects and to inform an intelligent view management of the user interface. While the vertical and horizontal orientation of attention is quite commonly used, e.g. derived from the orientation of the head, only knowledge about the distance allows for an intrinsic measurement of the location of the attention. This contribution reviews our latest results on detecting the location of attention in 3D space using binocular eye tracking.},
  author       = {Pfeiffer, Thies and Mattar, Nikita},
  booktitle    = {Workshop-Proceedings der Tagung Mensch \& Computer 2009: Grenzenlos frei!?},
  keyword      = {Gaze-based Interaction},
  publisher    = {Logos Berlin},
  title        = {{Benefits of locating overt visual attention in space using binocular eye tracking for mixed reality applications}},
  year         = {2009},
}

@inproceedings{73,
  abstract     = {The Semantic Web is about to become a rich source of knowledge whose potential will be squandered if it is not accessible for everyone. Intuitive interfaces like conversational agents are needed to better disseminate this knowledge, either on request or even proactively in a context-aware manner. This paper presents work on extending an existing conversational agent, Max, with abilities to access the Semantic Web via natural language communication.},
  author       = {Breuing, Alexa and Pfeiffer, Thies and Kopp, Stefan},
  booktitle    = {Proceedings of the Poster and Demonstration Session at the 7th International Semantic Web Conference (ISWC 2008)},
  editor       = {Bizer, Christian and Joshi, Anupam},
  title        = {{Conversational Interface Agents for the Semantic Web - a Case Study}},
  year         = {2008},
}

@inproceedings{74,
  abstract     = {Interaction in conversational interfaces strongly relies on the sys- tem's capability to interpret the user's references to objects via de- ictic expressions. Deictic gestures, especially pointing gestures, provide a powerful way of referring to objects and places, e.g., when communicating with an Embodied Conversational Agent in a Virtual Reality Environment. We highlight results drawn from a study on pointing and draw conclusions for the implementation of pointing-based conversational interactions in partly immersive Vir- tual Reality.},
  author       = {Pfeiffer, Thies and Latoschik, Marc Erich and Wachsmuth, Ipke},
  booktitle    = {Proceedings of the IEEE VR 2008},
  editor       = {Lin, Ming and Steed, Anthony and Cruz-Neira, Carolina},
  keyword      = {Multimodal Communication},
  pages        = {281--282},
  publisher    = {IEEE Press},
  title        = {{Conversational pointing gestures for virtual reality interaction: Implications from an empirical study}},
  doi          = {10.1109/vr.2008.4480801},
  year         = {2008},
}

@article{75,
  abstract     = {Tracking user's visual attention is a fundamental aspect in novel human-computer interaction paradigms found in Virtual Reality. For example, multimodal interfaces or dialogue-based communications with virtual and real agents greatly benefit from the analysis of the user's visual attention as a vital source for deictic references or turn-taking signals. Current approaches to determine visual attention rely primarily on monocular eye trackers. Hence they are restricted to the interpretation of two-dimensional fixations relative to a defined area of projection. The study presented in this article compares precision, accuracy and application performance of two binocular eye tracking devices. Two algorithms are compared which derive depth information as required for visual attention-based 3D interfaces. This information is further applied to an improved VR selection task in which a binocular eye tracker and an adaptive neural network algorithm is used during the disambiguation of partly occluded objects.},
  author       = {Pfeiffer, Thies and Latoschik, Marc Erich and Wachsmuth, Ipke},
  journal      = {JVRB - Journal of Virtual Reality and Broadcasting},
  keyword      = {human-computer interaction, object selection, virtual reality, gaze-based interaction, eye tracking},
  number       = {16},
  pages        = {1660},
  title        = {{Evaluation of binocular eye trackers and algorithms for 3D gaze interaction in virtual reality environments}},
  volume       = {5},
  year         = {2008},
}

@inproceedings{76,
  abstract     = {Of all senses, it is visual perception that is predominantly deluded in Virtual Realities. Yet, the eyes of the observer, despite the fact that they are the fastest perceivable moving body part, have gotten relatively little attention as an interaction modality. A solid integration of gaze, however, provides great opportunities for implicit and explicit human-computer interaction. We present our work on integrating a lightweight head-mounted eye tracking system in a CAVE-like Virtual Reality Set-Up and provide promising data from a user study on the achieved accuracy and latency.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Virtuelle und Erweiterte Realität - Fünfter Workshop der GI-Fachgruppe VR/AR},
  editor       = {Schumann, Marco and Kuhlen, Torsten},
  keyword      = {Gaze-based Interaction},
  pages        = {81--92},
  publisher    = {Shaker Verlag GmbH},
  title        = {{Towards Gaze Interaction in Immersive Virtual Reality: Evaluation of a Monocular Eye Tracking Set-Up}},
  year         = {2008},
}

@inproceedings{77,
  abstract     = {Emotions and interpersonal distances are identified as key aspects in social interaction. A novel Affective Computer-Mediated Communication (ACMC) framework has been developed making the interplay of both aspects explicit to facilitate social presence. In this ACMC framework, the displays can be arranged in virtual space manually or automatically. We expect that, according to empirical findings, the social relation as well as momentarily affective appraisals will influence this arrangement. The proposed concept extends from desktop devices to fully immersive Virtual Reality interfaces.},
  author       = {Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {Proceedings of the 11th International Workshop on Presence},
  editor       = {Spagnolli, Anna and Gamberini, Luciano},
  keyword      = {Mediated Communication},
  pages        = {275--279},
  publisher    = {CLEUP Cooperativa Libraria Universitaria Padova},
  title        = {{Social Presence: The Role of Interpersonal Distances in Affective Computer-Mediated Communication}},
  year         = {2008},
}

@inproceedings{78,
  abstract     = {People engaged in successful dialog have to share knowledge, e.g., when naming objects, for coordinating their actions. According to Clark (1996), this shared knowledge, the common ground, is explicitly established, particularly by negotiations. Pickering and Garrod (2004) propose with their alignment approach a more automatic and resource-sensitive mechanism based on priming. Within the collaborative research center (CRC) "Alignment in Communication" a series of experimental investigations of natural Face-to-face dialogs should bring about vital evidence to arbitrate between the two positions. This series should ideally be based on a common setting. In this article we review experimental settings in this research line and refine a set of requirements. We then present a flexible design called the Jigsaw Map Game and demonstrate its applicability by reporting on a first experiment on object naming.},
  author       = {Weiß, Petra and Pfeiffer, Thies and Schaffranietz, Gesche and Rickheit, Gert},
  booktitle    = {Cognitive Science 2007: Proceedings of the 8th Annual Conference of the Cognitive Science Society of Germany},
  editor       = {Zimmer, Hubert D. and Frings, C. and Mecklinger, Axel and Opitz, B. and Pospeschill, M. and Wentura, D.},
  keyword      = {Multimodal Communication},
  location     = {Saarbrücken, Germany},
  title        = {{Coordination in dialog: Alignment of object naming in the Jigsaw Map Game}},
  year         = {2008},
}

@misc{79,
  author       = {Meißner, Martin and Essig, Kai and Pfeiffer, Thies and Decker, Reinhold and Ritter, Helge},
  booktitle    = {Perception},
  keyword      = {In a novel approach we investigated choice processes using eye tracking to improve research instruments in marketing research. Choice-based conjoint analysis (CBC) is the most widely-used tool for investigating consumer preferences on the basis of choice tasks. While CBC is highly appreciated for its realism (Haaijer and Wedel, 2007), marketing researchers have highlighted that respondents are easily exposed to the problem of information overload (Green et al, 2001). The question how much information is being processed during choice processes and how preference measurement is affected remains an open research issue. We investigated choice processes using eye tracking in a CBC on-line consumer survey. We showed (i) that the extent to which information is processed is decreasing in later choice tasks, (ii) in how far information overload changes the pattern of eye movements, and (iii) how the difficulty of a choice task influences information processing.},
  pages        = {97--97},
  publisher    = {PION LTD},
  title        = {{Eye-tracking decision behaviour in choice-based conjoint analysis}},
  volume       = {37},
  year         = {2008},
}

@inproceedings{80,
  abstract     = {Für die Mensch-Maschine-Interaktion ist die Erfassung der Aufmerksamkeit des Benutzers von großem Interesse. Für Anwendungen in der Virtuellen Realität (VR) gilt dies insbesondere, nicht zuletzt dann, wenn Virtuelle Agenten als Benutzerschnittstelle eingesetzt werden. Aktuelle Ansätze zur Bestimmung der visuellen Aufmerksamkeit verwenden meist monokulare Eyetracker und interpretieren daher auch nur zweidimensionale bedeutungstragende Blickfixationen relativ zu einer Projektionsebene. Für typische Stereoskopie-basierte VR Anwendungen ist aber eine zusätzliche Berücksichtigung der Fixationstiefe notwendig, um so den Tiefenparameter für die Interaktion nutzbar zu machen, etwa für eine höhere Genauigkeit bei der Objektauswahl (Picking). Das in diesem Beitrag vorgestellte Experiment zeigt, dass bereits mit einem einfacheren binokularen Gerät leichter zwischen sich teilweise verdeckenden Objekten unterschieden werden kann. Trotz des positiven Ergebnisses kann jedoch noch keine uneingeschränkte Verbesserung der Selektionsleistung gezeigt werden. Der Beitrag schließt mit einer Diskussion nächster Schritte mit dem Ziel, die vorgestellte Technik weiter zu verbessern.},
  author       = {Pfeiffer, Thies and Donner, Matthias and Latoschik, Marc Erich and Wachsmuth, Ipke},
  booktitle    = {Virtuelle und Erweiterte Realität. 4. Workshop der GI-Fachgruppe VR/AR},
  editor       = {Latoschik, Marc Erich and Fröhlich, Bernd},
  keyword      = {Mensch-Maschine-Interaktion, Eyetracking, Virtuelle Realität},
  pages        = {113--124},
  publisher    = {Shaker},
  title        = {{Blickfixationstiefe in stereoskopischen VR-Umgebungen: Eine vergleichende Studie}},
  year         = {2007},
}

@article{81,
  abstract     = {Humans perceive and act within a visual world. This has become important even in
disciplines which are prima facie not concerned with vision and eye tracking is now used
in a broad range of domains. However, the world we are in is not two-dimensional, as
many experiments may convince us to believe. It is convenient to use stimuli in 2D or
21/2D and in most cases absolutely appropriate; but it is often technically motivated and
not scientifically.
To overcome these technical limitations we contribute results of an evaluation of different
approaches to calculate the depth of a fixation based on the divergence of the eyes
by testing them on different devices and within real and virtual scenarios.},
  author       = {Pfeiffer, Thies and Donner, Matthias and Latoschik, Marc Erich and Wachsmuth, Ipke},
  journal      = {Journal of Eye Movement Research},
  keyword      = {Gaze-based Interaction},
  pages        = {13},
  title        = {{3D fixations in real and virtual scenarios}},
  volume       = {Special issue: Abstracts of the ECEM 2007},
  year         = {2007},
}

@inproceedings{82,
  abstract     = {Im Rahmen der Entwicklung einer multimodalen Schnittstelle für die Mensch-Maschine Kommunikation konzentriert sich diese Arbeit auf die Interpretation von Referenzen auf sichtbare Objekte. Im Vordergrund stehen dabei Fragen zur Genauigkeit von Zeigegesten und deren Interaktion mit sprachlichen Ausdrücken. Die Arbeit spannt dabei methodisch einen Bogen von Empirie über Simulation und Visualisierung zur Modellbildung und Evaluation. In Studien zur deiktischen Objektreferenz wurden neben sprachlichen Äußerungen unter dem Einsatz moderner Motion Capturing Technik umfangreiche Daten zum deiktischen Zeigen erhoben. Diese heterogenen Daten, bestehend aus Tracking Daten, sowie Video und Audio Aufzeichnungen, wurden annotiert und mit eigens entwickelten interaktiven Werkzeugen unter Einsatz von Techniken der Virtuellen Realität integriert und aufbereitet. Die statistische Auswertung der Daten erfolgte im Anschluß mittels der freien Statistik-Software R. Die datengetriebene Modellbildung bildet die Grundlage für die Weiterentwicklung eines unscharfen, fuzzy-basierten, Constraint Satisfaction Ansatzes zur Interpretation von Objektreferenzen. Wesentliches Ziel ist dabei eine inkrementelle, echtzeitfähige Verarbeitung, die den Einsatz in direkter Mensch-Maschine Interaktion erlaubt. Die Ergebnisse der Studie haben über die Fragestellung hinaus Einfluss auf ein Modell zur Produktion von deiktischen Ausdrücken und direkte Konsequenzen für einschlägige Theorien zur deiktischen Referenz.},
  author       = {Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {Kognitionsforschung 2007 - Beiträge zur 8. Jahrestagung der Gesellschaft für Kognitionswissenschaft},
  editor       = {Frings, Christian and Mecklinger, Axel and Opitz, Bertram and Pospeschill, Markus and Wentura, Dirk and Zimmer, Hubert D.},
  keyword      = {multimodal communication},
  pages        = {109--110},
  publisher    = {Shaker Verlag},
  title        = {{Interpretation von Objektreferenzen in multimodalen Äußerungen}},
  year         = {2007},
}

@inproceedings{83,
  abstract     = {People engaged in successful dialog have to share knowledge, e.g., when naming objects, for coordinating their actions. According to Clark (1996), this shared knowledge, the common ground, is explicitly established, particularly by negotiations. Pickering and Garrod (2004) propose with their alignment approach a more automatic and resource-sensitive mechanism based on priming. Within the collaborative research center (CRC) “Alignment in Communication” a series of experimental investigations of natural face-to-face dialogs should bring about vital evidence to arbitrate between the two positions. This series should ideally be based on a common setting. In this article we review experimental settings in this research line and refine a set of requirements. We then present a flexible design called the Jigsaw Map Game and demonstrate its applicability by reporting on a first experiment on object naming.},
  author       = {Schaffranietz, Gesche and Weiß, Petra and Pfeiffer, Thies and Rickheit, Gert},
  booktitle    = {Kognitionsforschung 2007 - Beiträge zur 8. Jahrestagung der Gesellschaft für Kognitionswissenschaft},
  editor       = {Frings, Christian and Mecklinger, Axel and Opitz, Betram and Pospeschill, Markus and Wentura, Dirk and Zimmer, Hubert D.},
  keyword      = {Multimodal Communication},
  pages        = {41--42},
  publisher    = {Shaker Verlag},
  title        = {{Ein Experiment zur Koordination von Objektbezeichnungen im Dialog}},
  year         = {2007},
}

@inproceedings{84,
  abstract     = {The mediation of social presence is one of the most interesting challenges of modern communication technology. The proposed metaphor of Interactive Social Displays describes new ways of interactions with multi-/crossmodal interfaces prepared for a psychologically augmented communication. A first prototype demonstrates the application of this metaphor in a teleconferencing scenario.},
  author       = {Pfeiffer, Thies and Latoschik, Marc Erich},
  booktitle    = {IPT-EGVE 2007, Virtual Environments 2007, Short Papers and Posters},
  editor       = {Fröhlich, Bernd and Blach, Roland and Liere van, Robert},
  keyword      = {Mediated Communication},
  pages        = {41--42},
  publisher    = {Eurographics Association},
  title        = {{Interactive Social Displays}},
  year         = {2007},
}

@misc{85,
  abstract     = {The mediation of social presence is one of the most interesting challenges of modern communication technology. The proposed metaphor of Interactive Social Displays describes new ways of interactions with multi-/crossmodal interfaces prepared for a psychologically augmented communication. A first prototype demonstrates the application of this metaphor in a teleconferencing scenario.},
  author       = {Pfeiffer, Thies and Latoschik, Marc E.},
  keyword      = {Mediated Communication},
  title        = {{Interactive Social Displays}},
  year         = {2007},
}

@inbook{86,
  abstract     = {This chapter presents an original approach towards a detailed understanding of the usage of pointing gestures accompanying referring expressions. This effort is undertaken in the context of human-machine interaction integrating empirical studies, theory of grammar and logics, and simulation techniques. In particular, we take steps to classify the role of pointing in deictic expressions and to model the focussed area of pointing gestures, the so-called pointing cone. This pointing cone serves as a central concept in a formal account of multi-modal integration at the linguistic speech-gesture interface as well as in a computational model of processing multi-modal deictic expressions.},
  author       = {Kranstedt, Alfred and Lücking, Andy and Pfeiffer, Thies and Rieser, Hannes and Wachsmuth, Ipke},
  booktitle    = {Situated Communication},
  editor       = {Rickheit, Gert and Wachsmuth, Ipke},
  keyword      = {Multimodal Communication},
  pages        = {155--208},
  publisher    = {Mouton de Gruyter},
  title        = {{Deictic object reference in task-oriented dialogue}},
  doi          = {10.1515/9783110197747.155},
  year         = {2006},
}

@inbook{87,
  abstract     = {This contribution presents investigations of the usage of computer gene¬rated 3D stimuli for psycholinguistic experiments. In the first part, we introduce VDesigner. VDesigner is a visual programming environment that operates in two different modes, a design mode to implement the materials and the structure of an experiment, and a runtime mode to actually run the experiment. We have extended VDesigner to support interactive experimentation in 3D. In the second part, we de-scribe a practical application of the programming environment. We have replicated a previous 2½D study of the production of spatial terms in a 3D setting, with the objective of investigating the effect of the presentation modes (2½D vs. 3D) on the choice of the referential system. In each trial, on being presented with a scene, the participants had to verbally specify the position of a target object in relation to a reference object. We recorded the answers of the participants as well as their reac-tion times. The results suggest that stereoscopic 3D presentations are a promising technology to elicit a more natural behavior of participants in computer-based experiments.},
  author       = {Flitter, Helmut and Pfeiffer, Thies and Rickheit, Gert},
  booktitle    = {Situated Communication},
  editor       = {Rickheit, Gert and Wachsmuth, Ipke},
  keyword      = {Multimodal Communication},
  pages        = {127--153},
  publisher    = {Mouton de Gruyter},
  title        = {{Psycholinguistic experiments on spatial relations using stereoscopic presentation}},
  year         = {2006},
}

@inproceedings{88,
  abstract     = {Für die empirische Erforschung situierter natürlicher menschlicher Kommunikation sind wir auf die Akquise und Auswertung umfangreicher Daten angewiesen. Die Modalitäten, über die sich Menschen ausdrücken können, sind sehr unterschiedlich. Entsprechend heterogen sind die Repräsentationen, mit denen die erhobenen Daten für die Auswertung verfügbar gemacht werden können. Für eine Untersuchung des Zeigeverhaltens bei der Referenzierung von Objekten haben wir mit IADE ein Framework für die Aufzeichnung, Analyse und Simulation von Sprach-Gestik Daten entwickelt. Durch den Einsatz von Techniken aus der interaktiven VR erlaubt IADE die synchronisierte Aufnahme von Bewegungs-, Video- und Audiodaten und unterstützt einen iterativen Auswertungsprozess der gewonnenen Daten durch komfortable integrierte Revisualisierungen und Simulationen. Damit stellt IADE einen entscheidenden Fortschritt für unsere linguistische Experimentalmethodik dar.},
  author       = {Pfeiffer, Thies and Kranstedt, Alfred and Lücking, Andy},
  booktitle    = {Dritter Workshop Virtuelle und Erweiterte Realität der GI-Fachgruppe VR/AR},
  editor       = {Müller, Stefan and Zachmann, Gabriel},
  keyword      = {Multimodal Communication},
  pages        = {61--72},
  publisher    = {Shaker},
  title        = {{Sprach-Gestik Experimente mit IADE, dem Interactive Augmented Data Explorer}},
  year         = {2006},
}

@inproceedings{89,
  abstract     = {We present a collaborative approach towards a detailed understanding of the usage of pointing gestures accompanying referring expressions. This effort is undertaken in the context of human-machine interaction integrating empirical studies, theory of grammar and logics, and simulation techniques. In particular, we attempt to measure the precision of the focussed area of a pointing gesture, the so-called pointing cone. ne pointing cone serves as a central concept in a formal account of multi-modal integration at the linguistic speech-gesture interface as well as in a computational model of processing multi-modal deictic expressions.},
  author       = {Kranstedt, Alfred and Lücking, Andy and Pfeiffer, Thies and Rieser, Hannes and Wachsmuth, Ipke},
  booktitle    = {Gesture in Human-Computer Interaction and Simulation},
  editor       = {Gibet , Sylvie and Courty , Nicolas and Kamp , Jean-François},
  keyword      = {Multimodal Communication},
  pages        = {300--311},
  publisher    = {Springer},
  title        = {{Deixis: How to determine demonstrated objects using a pointing cone}},
  doi          = {10.1007/11678816_34},
  year         = {2006},
}

@inbook{90,
  abstract     = {Instructions play an important role in everyday communication, e.g. in task-oriented dialogs. Based on a (psycho-)linguistic theoretical background, which classifies instructions as requests, we conducted experiments using a cross-modal experimental design in combination with a reaction time paradigm in order to get insights in human instruction processing. We concentrated on the interpretation of basic single sentence instructions. Here, we especially examined the effects of the specificity of verbs, object names, and prepositions in interaction with factors of the visual object context regarding an adequate reference resolution. We were able to show that linguistic semantic and syntactic factors as well as visual context information context influence the interpretation of instructions. Especially the context information proves to be very important. Above and beyond the relevance for basic research, these results are also important for the design of human-computer interfaces capable of understanding natural language. Thus, following the experimental-simulative approach, we also pursued the processing of instructions from the perspective of computer science. Here, a natural language processing interface created for a virtual reality environment served as basis for the simulation of the empirical findings. The comparison of human vs. virtual system performance using a local performance measure for instruction understanding based on fuzzy constraint satisfaction led to further insights concerning the complexity of instruction processing in humans and artificial systems. Using selected examples, we were able to show that the visual context has a comparable influence on the performance of both systems, whereas this approach is limited when it comes to explaining some effects due to variations of the linguistic structure. In order to get deeper insights into the timing and interaction of the sub-processes relevant for instruction understanding and to model these effects in the computer simulation, more specific data on human performance are necessary, e.g. by using eye-tracking techniques. In the long run, such an approach will result in the development of a more natural and cognitively adequate human-computer interface.},
  author       = {Weiß, Petra and Pfeiffer, Thies and Eikmeyer, Hans-Jürgen and Rickheit, Gert},
  booktitle    = {Situated Communication},
  editor       = {Rickheit, Gert and Wachsmuth, Ipke},
  keyword      = {Multimodal Communication},
  pages        = {31--76},
  publisher    = {Mouton de Gruyter},
  title        = {{Processing Instructions}},
  year         = {2006},
}

@inproceedings{91,
  abstract     = {We describe an experiment to gather original data on geometrical aspects of pointing. In particular, we are focusing upon the concept of the pointing cone, a geometrical model of a pointing’s extension. In our setting we employed methodological and technical procedures of a new type to integrate data from annotations as well as from tracker recordings. We combined exact information on position and orientation with rater’s classifications. Our first results seem to challenge classical linguistic and philosophical theories of demonstration in that they advise to separate pointings from reference.},
  author       = {Kranstedt, Alfred and Lücking, Andy and Pfeiffer, Thies and Rieser, Hannes and Staudacher, Marc},
  booktitle    = {Proceedings of the brandial 2006 - The 10th Workshop on the Semantics and Pragmatics of Dialogue},
  editor       = {Schlangen, David and Fernández, Raquel},
  keyword      = {Multimodal Communication},
  pages        = {82--89},
  publisher    = {Universitätsverlag Potsdam},
  title        = {{Measuring and Reconstructing Pointing in Visual Contexts}},
  year         = {2006},
}

@inproceedings{92,
  abstract     = {Recently videoconferencing has been extended from human face-to-face communication to human machine interaction with Virtual Environments (VE)[6]. Relying on established videoconferencing (VC) protocol standards this thin client solution does not require specialised 3D soft- or hardware and scales well to multimedia enabled mobile devices. This would bring a whole range of new applications to the mobile platform. To facilitate our research in mobile interaction the Open Source project P@CE has been started to bring a fullfeatured VC client to the Pocket PC platform.},
  author       = {Weber, Matthias and Pfeiffer, Thies and Jung, Bernhard},
  booktitle    = {MOBILE HCI 05 Proceedings of the 7th International Conference on Human Computer Interaction with Mobile Devices and Services},
  editor       = {Tscheligi, Manfred and Bernhaupt, Regina and Mihalic, Kristijan},
  keyword      = {Mediated Communication},
  pages        = {351--352},
  publisher    = {ACM},
  title        = {{Pr@senZ - P@CE: Mobile Interaction with Virtual Reality}},
  year         = {2005},
}

@inproceedings{93,
  abstract     = {This paper presents an alternative to existing methods for remotely accessing Virtual Reality (VR) systems. Common solutions are based on specialised software and/or hardware capable of rendering 3D content, which not only restricts accessibility to specific platforms but also increases the barrier for non expert users. Our approach addresses new audiences by making existing Virtual Environments (VEs) ubiquitously accessible. Its appeal is that a large variety of clients, like desktop PCs and handhelds, are ready to connect to VEs out of the box. We achieve this combining established videoconferencing protocol standards with a server based interaction handling. Currently interaction is based on natural speech, typed textual input and visual feedback, but extensions to support natural gestures are possible and planned. This paper presents the conceptual framework enabling videoconferencing with collaborative VEs as well as an example application for a virtual prototyping system.},
  author       = {Pfeiffer, Thies and Weber, Matthias and Jung, Bernhard},
  booktitle    = {Theory and Practice of Computer Graphics 2005},
  editor       = {Lever, Louise and McDerby, Marc},
  keyword      = {Mediated Communication},
  pages        = {209--216},
  publisher    = {Eurographics Association},
  title        = {{Ubiquitous Virtual Reality: Accessing Shared Virtual Environments through Videoconferencing Technology}},
  year         = {2005},
}

@inproceedings{94,
  abstract     = {This paper describes the underlying concepts and the technical implementation of a system for resolving multimodal references in Virtual Reality (VR). In this system the temporal and semantic relations intrinsic to referential utterances are expressed as a constraint satisfaction problem, where the propositional value of each referential unit during a multimodal dialogue updates incrementally the active set of constraints. As the system is based on findings of human cognition research it also regards, e.g., constraints implicitly assumed by human communicators. The implementation takes VR related real-time and immersive conditions into account and adapts its architecture to well known scene-graph based design patterns by introducing a socalled reference resolution engine. Regarding the conceptual work as well as regarding the implementation, special care has been taken to allow further refinements and modifications to the underlying resolving processes on a high level basis.},
  author       = {Pfeiffer, Thies and Latoschik, M. E.},
  booktitle    = {Proceedings of the IEEE Virtual Reality 2004},
  editor       = {Ikei, Yasushi and Göbel, Martin and Chen, Jim},
  keyword      = {inform::Constraint Satisfaction inform::Multimodality inform::Virtual Reality ling::Natural Language Processing ling::Reference Resolution, Multimodal Communication},
  pages        = {35--42},
  title        = {{Resolving Object References in Multimodal Dialogues for Immersive Virtual Environments}},
  year         = {2004},
}

@inproceedings{95,
  abstract     = {This poster presents cognitive-motivated aspects of a technical system for the resolution of references to objects within an assembly-task domain. The research is integrated in the Collaborative Research Center SFB 360 which is concerned with situated artificial communicators. One application scenario consists of a task-oriented discourse between an instructor and a constructor who collaboratively build aggregates from a wooden toy kit (Baufix), or from generic CAD parts. In our current setting this scenario is embedded in a virtual reality (VR) installation, where the human user, taking the role of the instructor, guides the artificial constructor (embodied by the ECA Max) through the assembly process by means of multimodal task descriptions (see Figure 1). The system handles instructions like: Plug the left red screw from above in the middle hole of the wing and turn it this way. accompanied by coverbal deictic and mimetic gestures (see Latoschik, 2001).},
  author       = {Pfeiffer, Thies and Voss, Ian and Latoschik, Marc Erich},
  booktitle    = {Proceedings of the EuroCogSci03},
  editor       = {Schmalhofer, F. and Young, R.},
  keyword      = {inform::Multimodality inform::Virtual Reality ling::Reference Resolution ling::Natural Language Processing, Artificial Intelligence, Multimodal Communication},
  pages        = {426},
  publisher    = {Lawrence Erlbaum Associates Inc},
  title        = {{Resolution of Multimodal Object References using Conceptual Short Term Memory}},
  year         = {2003},
}

@misc{96,
  author       = {Pfeiffer, Thies},
  keyword      = {inform::Constraint Satisfaction inform::Multimodality inform::Virtual Reality ling::Natural Language Processing, Multimodal Communication},
  publisher    = {Faculty of Technology, University of Bielefeld},
  title        = {{Eine Referenzauflösung für die dynamische Anwendung in Konstruktionssituationen in der Virtuellen Realität}},
  year         = {2003},
}

@misc{97,
  abstract     = {When merging different knowledge bases one has to cope with the problem of classifying and linking concepts as well as the possibly heterogeneous representations the knowledge is expressed in. We are presenting an implementation that follows the Model Driven Architecture (MDA) [Miller and Mukerji, 2003] approach defined by the Object Management Group (OMG). Metamodels defined in the Unified Modeling Language (UML) are used to implement different knowledge representation formalisms. Knowledge is expressed as a Model instantiating the Metamodel. Integrating Metamodels are defined for merging knowledge distributed over different knowledge bases.},
  author       = {Pfeiffer, Thies and Voss, Ian},
  keyword      = {Artificial Intelligence},
  title        = {{Integrating Knowledge Bases Using UML Metamodels}},
  year         = {2003},
}

@inproceedings{98,
  abstract     = {This contribution describes a WWW-based multi-user system for concurrent virtual prototyping. A 3D scene of CAD parts is presented to the users in the web browser. By instructing the system using simple natural language commands, complex aggregates can be assembled from the basic parts. The current state of the assembly is instantly published to all system users who can discuss design choices in a chat area. The implementation builds on an existing system for virtual assembly made available as a web service. The client side components are fully implemented as Java applets and require no plugin for visualization of 3D content. Http tunneled messaging between web clients and server ensures system accessibility from any modern web browser even behind firewalls. The system is first to demonstrate natural language based virtual prototyping on the web.},
  author       = {Jung, Bernhard and Pfeiffer, Thies and Zakotnik, Jure},
  booktitle    = {Proceedings Structured Design of Virtual Environments and 3D-Components},
  editor       = {Geiger et al., C.},
  keyword      = {inform::Web inform::Internet inform::Collaborative Environments},
  pages        = {101--110},
  publisher    = {Shaker},
  title        = {{Natural Language Based Virtual Prototyping on the Web}},
  year         = {2002},
}


